Deployment的高明之处
有不同的更新策略
Recreate 策略在删除旧的Pod之后才开始创建新的Pod。如果不允许多个版本共存，使用这个，但会有短暂的不可用。
RollingUpdate 策略会渐进地删除旧的pod,于此同时创建新的pod.这是deployment默认使用的策略。如果支持多个版本共存，推荐使用这个。
 
我们来演示下deployment滚动升级的过程。
在演示之前我们先减慢滚动升级的速度，以方便我们观察
kubectl path deployment kubia -p '{"spec": {"minReadySeconds": 10} }'
使用path对于修改单个或少量资源属性非常有用，不需要在通过编辑器编辑，使用minReadySeconds 配置正常检查后多少时间才属于正常。
注：使用patch命令更改Deployment的自有属性，并不会导致pod的任何更新，因为pod模版并没有被修改。更改其他Deployment的属性，比如所需要的副本数或部署策略，也不会触发滚动升级，现有运行中的pod也不会受影响。
 
触发滚动升级
kubectl set image deployment kubia nodejs=luksa/kubia:v2
执行完成后pod模版的镜像会被更改为luksa/kubia:v2
 
deployment的优点
使用控制器接管整个升级过程，不用担心网络中断。
仅仅更改pod模版即可。
注：如果Deployment中的pod模版引用了一个ConfigMap(或secret)，那么更改ConfigMap字眼本身将不会触发升级操作。如果真的需要修改应用程序的配置并想触发更新的话，可以通过创建一个新的ConfigMap并修改pod模版引用新的ConfigMap.
Deployment背后完成的升级过程和kubectl rolling-update命令相似。
一个新的ReplicaSet会被创建然后慢慢扩容，同时之前版本的Replicaset会慢慢缩容至0
 
deployment的另外一个好处是可以回滚
kubectl rollout undo deployment kubia
deployment会被回滚到上一个版本
undo命令也可以在滚动升级过程中运行，并直接停止滚动升级。在升级过程中一创建的pod会被删除并被老版本的pod替代。
显示deployment的滚动升级历史
kubectl rollout history deployment kubia
reversion change-cause
2 kubectl set image deployment kubia nodejs=luksa/kubia:v2
3 kubectl set image deployment kubia nodejs=luksa/kubia:v3
还记得创建Deployment的时候--record 参数吗？如果不给这个参数，版本历史中的Change-cause这一栏会空。这也会使用户很难辨别每次的版本做了哪些修改。
回滚到一个特定的Deployment版本
kubectl rollout undo deployment kubia --to-reversion=1
这些历史版本的replicaset都用特定的版本号保存Deployment的完整的信息，所以不应该手动删除ReplicaSet。如果删除会丢失Deployment的历史版本记录而导致无法回滚。
ReplicaSet默认保留最近的两个版本，可以通过制定Deployment的reversionHistoryLimit属性来限制历史版本数量。apps/v1beta2 版本的Deployment默认值为10.
 
控制滚动更新的速率
maxSurge 最多超过期望副本数多少个pod，默认为25%，可以设置为整数
maxUanavailable 最多允许期望副本数内多少个pod为不可用状态。
看图比较容易理解
 
暂停滚动更新：
kubectl rollout pause deployment kubia
 
恢复滚动更新：
kubectl rollout resume deployment kubia
 
阻止出错版本的滚动升级
使用minReadySeconds属性指定至少要成功运行多久之后，才能将其视为可用。在pod可用之前，滚动升级的过程不会继续。是由maxSurge属性和maxUanavailable属性决定。
例子：
复制代码
apiVersion: apps/v1beta1
kind: Deployment
metadata:
　　name: kubia
spec:
　　replicas:3
　　minReadySeconds: 10
　　strategy:
　　　　rollingUpdate:
　　　　　　maxSurge: 1
　　　　　　maxUnavilable: 0
　　　　type: RollingUpdate
　　template:
　　　　metadata:
　　　　　　name: kubia
　　　　　　labels:
　　　　　　　　app: kubia
　　　　spec:
　　　　　　containers:
　　　　　　-    image: luksa/kubia:v3
　　　　　　　　　name: nodejs
　　　　　　readinessProbe:
　　　　　　　　periodSeconds:  1     1秒探测一次
　　　　　　　　httpGet:
　　　　　　　　　　path: /
　　　　　　　　　　port: 8080
复制代码
kubectl apply -f kubia-deployment-v3-with-readinesscheck.yaml
kubectl rollout status deployment kubia
就绪探针是如何阻止出错版本的滚动升级的
首先luksa/kubia:v3镜像应用前5个应用是正常反馈，后面会500状态返回。因此pod会从service的endpoint中移除。当执行curl时，pod已经被标记为未就绪。这就解释了为什么请求不会到新的pod上了。
使用kubectl rollout status deployment kubia查看状体，发现只显示一个新副本启动，之后滚动升级便没有进行下去，因为新的pod一直处于不可用状态。即使变为就绪状态后，也至少需要保持10秒，才是真正可用。在这之前滚动升级过程将不再创建任何新的pod，因为当前maxUnavailable属性设置为0，所以不会删除任何原始的pod。如果没有定义minReadySeconds，一旦有一次就绪探针调用成功，便会认为新的pod已经处于可用状态。因此最好适当的设置minReadySeconds.另外就绪探针默认间隔为1秒。
deployment资源介绍完结。
